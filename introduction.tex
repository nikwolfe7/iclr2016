\section{Introduction}
Neural network pruning algorithms were first popularized by \cite{sietsma1988neural} as a mechanism to determine the size of network required to solve a particular problem. To this day, network design and optimal pruning remain inherently difficult tasks. For problems which cannot be solved using linear threshold units alone, \cite{baum1989size} demonstrate there is no way to precisely determine the appropriate size of a neural network a priori given any random set of training instances. Using too few neurons inhibits learning, and so in practice it is common to attempt to over-parameterize networks initially using a large number of hidden units and weights. However, as \cite{chauvin1990generalization} writes, this approach can lead to overfitting as the network starts to latch onto idiosyncrasies in the training data. 


Pruning algorithms, as comprehensively surveyed by \cite{reed1993pruning}, are a useful set of heruristics designed to identify and remove network parameters which do not contribute significantly to the output of a network and potentially inhibit generalization performance. At the time of Reed's writing, reducing network size was also a practical concern, as smaller networks are preferable in situations where computational resources are scarce. In this paper we are particularly concerned with application domains in which space is limited and network size constraints must be imposed with minimal reduction in performance.


The generalization performance of neural networks has been well studied, and apart from pruning algorithms many heuristics have been proposed to avoid overfitting, such as dropout (\cite{srivastava2014dropout}), maxout (\cite{goodfellow2013maxout}), and cascade correlation (\cite{fahlman1989cascade}), among others. However, these algorithms do not explicitly prioritize the reduction of network memory footprint as part of their optimization criteria per se, (although cascade correlation holds promise in this regard.) Computer memory size and computational resources have improved so much since the introduction of pruning algorithms that space complexity has become much less of a concern. The proliferation of Cloud-based computing resources has furthermore enabled mobile and embedded devices to leverage the power of massive data and computing centers remotely. It is of course conceivable that performance-critical applications running on low-resource devices could benefit immensely from the ability to use powerful neural networks locally. 


Unfortunately at present there are few if any mechanisms to shrink neural networks down in order to meet an externally imposed constraint on byte-size in memory. Arguably one could accomplish this using any number of off-the-shelf pruning algorithms, such as Skeletonization (\cite{mozer1989skeletonization}), Optimal Brain Damage (\cite{lecun1989optimal}), or later variants such as Optimal Brain Surgeon (\cite{hassibi1993second}). In fact, we borrow much of our inspiration from these antecedent algorithms, with one major variation. 


The aforementioned strategies all focus on the targeting and removal of \textit{weight} parameters. Scoring and ranking individual weight parameters is computationally expensive, and generally speaking the removal of a single weight from a large network is a drop in the bucket in terms of reducing a network's core memory footprint. We therefore train our sights on the ranking and removal of entire neurons along with their associated weight parameters. We argue that this is more efficient computationally as well as practically in terms of helping us quickly reach a memory size target. Our approach also attacks the angle of giving downstream applications a realistic expectation of the minimal increase in error resulting from the removal of a requested percentage of neurons from a trained network. Such trade-offs are unavoidable, but performance impacts can be limited if a principled approach is used to find candidate neurons for removal. 


