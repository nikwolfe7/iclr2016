\section{Introduction}
Neural network pruning algorithms were first popularized by \cite{sietsma1988neural} as a mechanism to determine the size of network required to solve a particular problem. To this day, network design and optimal pruning remain inherently difficult tasks. For problems which cannot be solved using linear threshold units alone, \cite{baum1989size} demonstrate there is no way to precisely determine the appropriate size of a neural network a priori given any random set of training instances. Using too few neurons inhibits learning, and so in practice it is common to attempt to over-parameterize networks initially using a large number of hidden units and weights. As \cite{chauvin1990generalization} shows, this approach can lead to overfitting as the network starts to latch onto the idiosyncrasies in the training data. Pruning algorithms, as surveyed by \cite{reed1993pruning}, are a useful set of heruristics designed to remove network parameters which inhibit the generalization performance of a trained network. Smaller networks are also ideal in situations where computational resources are scarce.
\\
Improving generalization performance has been well studied, and many heuristics exist to avoid overfitting, such as dropout (), maxout et al, and  