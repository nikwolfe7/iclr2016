\section{Second Derivative Gradient Terms}
Given a neuron $n$ with output $a_i$, and outgoing weights $\left[ w_{i,1}, w_{i,2}, \ldots w_{i,j} \right]$, the input $x$ to each of its $j$ forward-connected neurons is given by:
\begin{equation}
x_j = \sum_i\left(w_{i,j} \cdot a_i\right)
\end{equation}
For simplicity's sake, we will drop the index variable $i$ and examine only the connection between the output $a$ of neuron $n$ and each of its $j$ connections to the next forward layer. So the contribution $c_j$ from neuron $n$ to the input of each forward-connected neuron is given by:
\begin{equation}
c_j = w_j \cdot a
\end{equation} 
Where $w_j$ is the weight connecting the output $a$ from $n$ to the input of the $j$th neuron. We will denote the error function of an optimally trained network as $E$. The second-derivative of $E$ with respect to the output of neuron $n$ is given by: 
\begin{equation}
\frac{d^2E}{da^2} = \frac{d}{da}\frac{dE}{da} = \frac{d}{da}\sum_j\left(\frac{dE}{dc_j}\cdot\frac{dc_j}{da}\right) = \sum_j\left(\frac{d}{da}\frac{dE}{dc_j}\cdot\frac{dc_j}{da}\right)
\end{equation}
Which states that the 2nd derivative of $E$ with respect to $a$ is the sum of the 2nd derivative terms of all outgoing connections.  

\subsection{Second Derivative Back-Propagation}

Name and network definitions:\\[5pt]
\begin{align}
E &= \frac{1}{2}\sum\limits_i (\Out i 0 - \Target i)^2 &
\Out i m &= \sigma(\Input i m) &
\Input i m &= \sum\limits_j {\Weight j i m}{ \Out j {m + 1}} &
\Con j i m = \Weight j i m \Out j {m+1}
\end{align}
Superscripts represent the index of the layer of the network in question, with 0 representing the output layer. $E$ is the squared-error network cost function. $\Out i m$ is the $i$th output in layer $m$ generated by the activation function $\sigma$, which in this paper is is the standard logistic sigmoid. $\Input i m$ is the weighted sum of inputs to the $i$th neuron in the $m$th layer, and $\Con j i m$ is the contribution of the $j$th neuron in the $m+1$ layer to the input of the $i$th neuron in the $m$th layer. 