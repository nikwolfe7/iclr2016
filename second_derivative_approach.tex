\subsection{Quadratic Approximation Approach}
\input{diagram.tex}
We define the following network terminology here which will be used in this section and all subsequent sections  unless stated otherwise. Figure ????? can be used as a reference to the terminology defined here:

\begin{align}
E &= \frac{1}{2}\sum\limits_i (\Out i 0 - \Target i)^2 &
\Out i m &= \sigma(\Input i m) &
\Input i m &= \sum\limits_j {\Weight j i m}{ \Out j {m + 1}} &
\Con j i m = \Weight j i m \Out j {m+1}\label{eq:term}
\end{align}
Superscripts represent the index of the layer of the network in question, with 0 representing the output layer. $E$ is the squared-error network cost function. Note that we are dropping the $E_n$ notation used previously as the subsequent discussion is insusceptible to the data instances. $\Out i m$ is the $i$th output in layer $m$ generated by the activation function $\sigma$, which in this paper is is the standard logistic sigmoid. $\Input i m$ is the weighted sum of inputs to the $i$th neuron in the $m$th layer, and $\Con j i m$ is the contribution of the $j$th neuron in the $(m+1)$th layer to the input of the $i$th neuron in the $m$th layer. $\Weight j i m$ is the weight between the $j$th neuron in the $(m+1)$th layer and the $i$th neuron in the $m$th layer.

As seen in equation \ref{eq:ts3}, $\Delta E_{n,k}$ which can now be represented as $\Delta E_{k}$ is the quadratic approximation of the change in error due to the $k$th neuron being turned off which becomes the basis of our ranking in this approach. The quadratic term in equation \ref{eq:ts3} requires some discussion which we provide here. A more detailed and step-by-step mathematical derivation can be found in the appendix.

Let us reproduce equation \ref{eq:ts3} in our new terminology here: 
\begin{align}
\Delta E_{k} = - o_k\cdot \left.\pdv{E}{{\Out j {m+1}}}\right|_{o_k} + 0.5\cdot o_k^2\cdot \left.\pdv[2]{E}{{\Out j {m+1}}}\right|_{o_k}\
\end{align}

The second term here involves the second-order gradient which represents the second-order change in error with respect to the output of a given neuron $o_j$ in the $(m+1)$th layer. This term can be generated by performing back-propagation using second derivatives. A full derivation of the second derivative back-propagation can be found in the appendix. We will quote some results from the derivation here. The second-order derivative term can be represented as:

\begin{align}
\pdv[2]{E}{{\Out j {m+1}}} &= \sum_i
\pdv[2]{E}{{\Con j i m}} \left({\Weight j i m}\right)^2
\end{align} 

Here,$\Con j i m$ is one of the component terms of $\Input i m$, as follows from the equations in \ref{eq:term}. Hence, it can be easily proved that (full proof in appendix):
\begin{align}
\pdv[2]{E}{{\Con j i m}} = \pdv[2]{E}{{\Input i m}}
\end{align}

Now, the value of $\Input i m$ can be easily calculated through the steps of the second-order back-propagation using Chain Rule. The full derivation can again, be found in the appendix.
\begin{align}
\pdv[2]{E}{{\Input i m}}=\pdv[2]{E}{{\Out i m}} \left(\sigma^{\prime}\left({\Input i m}\right)\right)^2
+
\pdv{E}{{\Out i m}}\sigma^{\prime\prime}\left(\Input i m\right)
\end{align}
