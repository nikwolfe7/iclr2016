\pagebreak
\section{Second Derivative Back-Propagation}
Name and network definitions:\\[5pt]
\begin{align}
E &= \frac{1}{2}\sum\limits_i (\Out i 0 - \Target i)^2 &
\Out i m &= \sigma(\Input i m) &
\Input i m &= \sum\limits_j {\Weight j i m}{ \Out j {m + 1}} &
\Con j i m = \Weight j i m \Out j {m+1}
\end{align}
Superscripts represent the index of the layer of the network in question, with 0 representing the output layer. $E$ is the squared-error network cost function. $\Out i m$ is the $i$th output in layer $m$ generated by the activation function $\sigma$, which in this paper is is the standard logistic sigmoid. $\Input i m$ is the weighted sum of inputs to the $i$th neuron in the $m$th layer, and $\Con j i m$ is the contribution of the $j$th neuron in the $m+1$ layer to the input of the $i$th neuron in the $m$th layer. 
\subsection{First and Second Derivatives} 
The first and second derivatives of the cost function with respect to the outputs:
\begin{align}
\pdv{E}{\Out i 0} &= \Out i 0 - \Target i \label{cost_func_derivative}
\end{align}
\begin{align}
\pdv[2]{E}{{\Out i 0}} &= 1\label{cost_func_2nd_derivative}
\end{align}
\\[5pt]
The first and second derivatives of the sigmoid function in forms depending only on the output:
\begin{align}
\sigma\prime(x) &= \sigma(x)\left(1 - \sigma(x)\right)\label{sigmoid_derivative} 
\\
\sigma\prime\prime(x) &= \sigma\prime(x)\left(1 - 2\sigma(x)\right) \label{sigmoid_2nd_derivative}
\end{align}
The second derivative of the sigmoid is easily derived from the first derivative:
\begin{align}
\sigma\prime(x) &= \sigma(x)\left(1 - \sigma(x)\right)
\\
\sigma\prime\prime(x) &= \dv{}{x}
\underbrace{\sigma(x)}_{f(x)}
\underbrace{\left(1 - \sigma(x)\right)}_{g(x)}
\\
\sigma\prime\prime(x) &= f\prime(x)g(x) + f(x)g\prime(x)
\\
\sigma\prime\prime(x) &= \sigma\prime(x)(1-\sigma(x)) - \sigma(x)\sigma\prime(x)
\\
\sigma\prime\prime(x) &= \sigma\prime(x) - 2\sigma(x)\sigma\prime(x)
\\
\sigma\prime\prime(x) &= \sigma\prime(x)(1 - 2\sigma(x))
\end{align}
And for future convenience: 
\begin{align}
\dv{\Out i m}{\Input i m} &= 
\dv{}{\Input i m}\left({\Out i m} = \sigma(\Input i m)\right) 
\\
&= \left(\Out i m\right)\left(1 - \Out i m\right)
\\
&= \sigma\prime\left(\Input i m\right)
\\
\dv[2]{{\Out i m}}{{\Input i m}} &=
\dv{}{{\Input i m}}\left(\dv{\Out i m}{\Input i m} = \left(\Out i m\right)\left(1 - \Out i m\right)\right)
\\
&= \left(\Out i m\left(1 - \Out i m\right)\right)\left(1 - 2\Out i m\right)
\\
&= \sigma\prime\prime\left(\Input i m\right)
\end{align}
\\[5pt]Derivative of the error with respect to the $i$th neuron's input $\Input i 0$ in the output layer:
\begin{align}
\pdv{E}{\Input i 0} &= \pdv{E}{\Out i 0} \pdv{\Out i 0}{\Input i 0} 
\\
&= \underbrace{\left(\Out i 0 - \Target i\right)}_{\text{from} \ (\ref{cost_func_derivative})} \underbrace{\sigma\left(\Input i 0\right)\left(1 - \sigma\left(\Input i 0\right)\right)}
_{\text{from} \ (\ref{sigmoid_derivative})}
\\
&= \left(\Out i 0 - \Target i\right)\left(\Out i 0 \left(1 - \Out i 0\right)\right)
\\
&= \left(\Out i 0 - \Target i\right)\sigma\prime\left(\Input i 0\right)\label{dedx}
\end{align}
\\[5pt]Second derivative of the error with respect to the $i$th neuron's input $\Input i 0$ in the output layer:
\begin{align}
\pdv[2]{E}{{\Input i 0}} &= \pdv{}{\Input i 0}
\left(\pdv{E}{\Out i 0}\pdv{\Out i 0}{\Input i 0}\right) 
\\
&= \pdv{{^2}E}{{\Input i 0}{\Out i 0}}\pdv{\Out i 0}{\Input i 0} + \pdv{E}{\Out i 0}\pdv[2]{\Out i 0}{{\Input i 0}}
\\
&= \pdv{{^2}E}{{\Input i 0}{\Out i 0}}\underbrace{\left(\Out i 0 \left(1 - \Out i 0\right)\right)}_{{\text{from} \ (\ref{sigmoid_derivative})}} + \underbrace{\left(\Out i 0 - \Target i\right)}_{\text{from} \ (\ref{cost_func_derivative})}\underbrace{\left(\Out i 0 \left(1 - \Out i 0\right)\right)\left(1 - 2\Out i 0\right) }_{\text{from}\ (\ref{sigmoid_2nd_derivative})}
\\
\left(\pdv{{^2}E}{{\Input i 0}{\Out i 0}}\right) &= \pdv{}{\Input i 0}\pdv{E}{\Out i 0} = \pdv{}{\Input i 0}\underbrace{\left(\Out i 0 - \Target i\right)}_{\text{from} \ (\ref{cost_func_derivative})} = \pdv{\Out i 0}{\Input i 0} = \underbrace{\left(\Out i 0 \left(1 - \Out i 0\right)\right)}_{{\text{from} \ (\ref{sigmoid_derivative})}} 
\\
\pdv[2]{E}{{\Input i 0}}&= \left(\Out i 0 \left(1 - \Out i 0\right)\right)^2 + \left(\Out i 0 - \Target i\right)\left(\Out i 0 \left(1 - \Out i 0\right)\right)\left(1 - 2\Out i 0\right) 
\\
&= \sigma\prime\left(\Input i 0\right)^2 + \left(\Out i 0 - \Target i\right)\sigma\prime\prime\left(\Input i 0\right)\label{d2edx2}
\end{align}
\\[5pt]First derivative of the error with respect to a single input contribution $\Con j i 0$ from neuron $j$ to neuron $i$ with weight $\Weight j i 0$ in the output layer:
\begin{align}
\pdv{E}{\Con j i 0} &= 
\pdv{E}{\Out i 0}
\pdv{\Out i 0}{\Input i 0}
\pdv{\Input i 0}{\Con j i 0}
\\
&= \underbrace{\left(\Out i 0 - \Target i \right)}_{\text{from} \ (\ref{cost_func_derivative})} \underbrace{\left(\Out i 0 \left(1 - \Out i 0\right) \right)}_{\text{from} \ (\ref{sigmoid_derivative})} \pdv{\Input i 0}{\Con j i 0} 
\\
\left( \pdv{\Input i m}{\Con j i m}\right) &= \pdv{}{\Con j i m}\left(\Input i m = \sum_j\Weight j i m\Out j {m+1} \right) = \pdv{}{\Con j i m} \left(\Con j i m + k \right) = 1\label{dxdc} 
\\
\pdv{E}{\Con j i 0}&= \left(\Out i 0 - \Target i \right) \left(\Out i 0 \left(1 - \Out i 0\right) \right)
\\
&= \underbrace{\left(\Out i 0 - \Target i \right) \sigma\prime\left(\Input i 0\right)\label{dedc}}
_{\text{from} \ (\ref{dedx})} 
\\
\pdv{E}{\Con j i 0} &= \pdv{E}{\Input i 0}
\end{align}
Second derivative of the error with respect to a single input contribution $\Con j i 0$:
\begin{align}
\pdv[2]{E}{{\Con j i 0}} &=
\pdv{}{\Con j i 0} 
\left(\pdv{E}{\Con j i 0} = 
\underbrace{\left(\Out i 0 - \Target i \right) \sigma\prime\left(\Input i 0\right)}
_{\text{from} \ (\ref{dedc})}
\right)
\\
&=\pdv{}{\Con j i 0}\left(\sigma\left(\Input i 0\right) - \Target i \right) \sigma\prime\left(\Input i 0\right)
\\
&=\pdv{}{\Con j i 0}\left(\sigma\left(\sum\limits_j {\Weight j i m}{ \Out j {m + 1}}\right) - \Target i \right) \sigma\prime\left(\sum\limits_j {\Weight j i m}{ \Out j {m + 1}}\right)
\\
&=\pdv{}{\Con j i 0}\left(\sigma\left(\sum\limits_j {\Con j i 0}\right) - \Target i \right) \sigma\prime\left(\sum\limits_j {\Con j i 0}\right)
\\
&=\pdv{}{\Con j i 0}
\underbrace{\left(\sigma\left({\Con j i 0} + k\right) - \Target i \right)}
_{f\left(\Con j i 0\right)}
\underbrace{\sigma\prime\left({\Con j i 0} + k\right)}
_{g\left(\Con j i 0\right)}
\\
&=f\prime\left(\Con j i 0\right)g\left(\Con j i 0\right) + f\left(\Con j i 0\right)g\prime\left(\Con j i 0\right)
\\
&=\sigma\prime\left({\Con j i 0} + k\right)\sigma\prime\left({\Con j i 0} + k\right) + 
\left(\sigma\left({\Con j i 0} + k\right) - \Target i \right)\sigma\prime\prime\left({\Con j i 0} + k\right)
\\
&=\sigma\prime\left({\Con j i 0} + k\right)^2 + 
\left(\Out i 0 - \Target i \right)\sigma\prime\prime\left({\Con j i 0} + k\right)
\\
&\left(\Con j i 0 + k = \sum_j{\Con j i 0} = \sum\limits_j {\Weight j i m}{ \Out j {m + 1}} = \Input i 0 \right)
\\
\pdv[2]{E}{{\Con j i 0}}&=
\underbrace{\sigma\prime\left(\Input i 0\right)^2 + 
\left(\Out i 0 - \Target i \right)\sigma\prime\prime\left(\Input i 0\right)}
_{\text{from} \ (\ref{d2edx2})}
\\
\pdv[2]{E}{{\Con j i 0}} &= \pdv[2]{E}{{\Input i 0}}
\end{align}
\subsubsection{Summary Of Output Layer Derivatives}
\begin{align}
&\pdv{E}{\Out i 0} = \Out i 0 - \Target i 
&
\pdv[2]{E}{(\Out i 0)} = 1
\end{align}
\begin{align}
&\pdv{E}{\Input i 0} = \left(\Out i 0 - \Target i\right)\sigma\prime\left(\Input i 0\right)
& 
\pdv[2]{E}{{\Input i 0}} = \sigma\prime\left(\Input i 0\right)^2 + \left(\Out i 0 - \Target i\right)\sigma\prime\prime\left(\Input i 0\right)
\end{align}
\begin{align}
&\pdv{E}{{\Con j i 0}} = \pdv{E}{{\Input i 0}}
&
\pdv[2]{E}{{\Con j i 0}} = \pdv[2]{E}{{\Input i 0}}
\end{align}
\subsubsection{Hidden Layer Derivatives}
The first derivative of the error with respect to a neuron with output $\Out j 1$ in the first hidden layer:
\begin{align}
\pdv{E}{\Out j 1} &= 
\pdv{E}{\Out i 0}
\pdv{\Out i 0}{\Input i 0}
\pdv{\Input i 0}{\Con j i 0}
\pdv{\Con j i 0}{\Out j 1}
= 
\underbrace{\left(\Out i 0 - \Target i\right)\sigma\prime\left(\Input i 0\right)}
_{\text{from} \ (\ref{dedx})}
\Weight j i 0
\\
&\left(\pdv{{\Con j i m}}{{\Out j {m+1}}} = \pdv{}{{\Out j {m+1}}}\left(\Con j i m = \Weight j i m\Out j {m+1}\right) = \Weight j i m\right)\label{dcdo}
\\
\pdv{E}{\Out j 1} &= \pdv{E}{\Input i 0}\Weight j i 0
\end{align}
The second derivative of the error with respect to a neuron with output $\Out j 1$ in the first hidden layer:
\begin{align}
\pdv[2]{E}{{\Out j 1}} &= 
\pdv{}{\Out j 1}\left(
\pdv{E}{\Out j 1}
\right)
\\
&= \pdv{}{\Out j 1}\left(
\pdv{E}{\Input i 0}\Weight j i 0
\right)
\end{align}
%
%\begin{align}
%\pdv[2]{E}{\Out j 1} &= \pdv[2]{E}{\Out j 1}{\Out i 0}
%\cdot\pdv{\Out i 0}{\Input i 0}\pdv{\Input i 0}{\Con j i 0}
%\cdot\pdv{\Con j i 0}{\Out j 1}
%+ \pdv{E}{\Out i 0}
%\cdot\pdv[2]{\Out i 0}{\Out j 1}{\Input i 0}
%\cdot\pdv{\Input i 0}{\Con j i 0}
%\cdot\pdv{\Con j i 0}{\Out j 1} \notag \\
%&\quad+ \pdv{E}{\Out i 0}
%\cdot\pdv{\Out i 0}{\Input i 0}
%\cdot\pdv[2]{\Input i 0}{\Out j 1}{\Con j i 0}
%\cdot\pdv{\Con j i 0}{\Out j 1}
%+ \pdv{E}{\Out i 0}
%\cdot\pdv{\Out i 0}{\Input i 0}
%\cdot\pdv{\Input i 0}{\Con j i 0}
%\cdot\pdv[2]{\Con j i 0}{\Out j 1} \\
%&= \left(\sigma'(\Input i 0)\Weight j i 0\right)^2 + \left(\Out i 0 - \Target i\right)\sigma''(\Input i 0)\Weight j i 0^2
%\end{align}
