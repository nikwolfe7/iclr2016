\pagebreak
\section{First and Second Derivatives} 
The first and second derivatives of the cost function with respect to the output:
\begin{align}
\pdv{E}{\Out i 0} &= \Out i 0 - \Target i \label{cost_func_derivative}
\end{align}
\begin{align}
\pdv[2]{E}{(\Out i 0)} &= 1\label{cost_func_2nd_derivative}
\end{align}
\\[5pt]
The first and second derivatives of the sigmoid function in forms depending only on the output:
\begin{align}
\sigma\prime &= \sigma\left(1 - \sigma\right)\label{sigmoid_derivative} 
\\
\sigma\prime\prime &= \sigma\prime\left(1 - 2\sigma\right) \label{sigmoid_2nd_derivative}
\end{align}
And for future convenience: 
\begin{align}
\pdv{\Out i m}{\Input i m} &= 
\pdv{}{\Input i m}\left({\Out i m} = \sigma(\Input i m)\right) = \left(\Out i m\right)\left(1 - \Out i m\right)
\\
\pdv[2]{{\Out i m}}{{\Input i m}}
\end{align}
\\[5pt]Derivative of the error with respect to the $i$th neuron's input $\Input i 0$ in the output layer:
\begin{align}
\pdv{E}{\Input i 0} &= \pdv{E}{\Out i 0} \pdv{\Out i 0}{\Input i 0} 
\\
&= \underbrace{\left(\Out i 0 - \Target i\right)}_{\text{from} \ (\ref{cost_func_derivative})} \underbrace{\left(\sigma\left(\Input i 0\right)\left(1 - \sigma\left(\Input i 0\right)\right)\right)}_{\text{from} \ (\ref{sigmoid_derivative})}
\\
&= \left(\Out i 0 - \Target i\right)\left(\Out i 0 \left(1 - \Out i 0\right)\right)
\\
&= \left(\Out i 0 - \Target i\right)\left(\sigma\prime\right)
\end{align}
\\[5pt]Second derivative of the error with respect to the $i$th neuron's input $\Input i 0$ in the output layer:
\begin{align}
\pdv[2]{E}{(\Input i 0)} &= \pdv{}{\Input i 0}
\left(\pdv{E}{\Out i 0}\pdv{\Out i 0}{\Input i 0}\right) 
\\
&= \pdv{{^2}E}{{\Input i 0}{\Out i 0}}\pdv{\Out i 0}{\Input i 0} + \pdv{E}{\Out i 0}\pdv[2]{\Out i 0}{{\Input i 0}}
\\
&= \pdv{{^2}E}{{\Input i 0}{\Out i 0}}\underbrace{\left(\Out i 0 \left(1 - \Out i 0\right)\right)}_{{\text{from} \ (\ref{sigmoid_derivative})}} + \underbrace{\left(\Out i 0 - \Target i\right)}_{\text{from} \ (\ref{cost_func_derivative})}\underbrace{\left(\Out i 0 \left(1 - \Out i 0\right)\right)\left(1 - 2\Out i 0\right) }_{\text{from}\ \ref{sigmoid_2nd_derivative}}
\\
\left(\pdv{{^2}E}{{\Input i 0}{\Out i 0}}\right) &= \pdv{}{\Input i 0}\pdv{E}{\Out i 0} = \pdv{}{\Input i 0}\underbrace{\left(\Out i 0 - \Target i\right)}_{\text{from} \ (\ref{cost_func_derivative})} = \pdv{\Out i 0}{\Input i 0} = \underbrace{\left(\Out i 0 \left(1 - \Out i 0\right)\right)}_{{\text{from} \ (\ref{sigmoid_derivative})}} 
\\
\pdv[2]{E}{(\Input i 0)}&= \left(\Out i 0 \left(1 - \Out i 0\right)\right)^2 + \left(\Out i 0 - \Target i\right)\left(\Out i 0 \left(1 - \Out i 0\right)\right)\left(1 - 2\Out i 0\right) 
\\
&= \left(\sigma\prime\right)^2 + \left(\Out i 0 - \Target i\right)\left(\sigma\prime\prime\right)
\end{align}
\\[5pt]First derivative of the error with respect to a single input contribution $\Con j i 0$ from neuron $j$ to neuron $i$ with weight $\Weight j i 0$ in the output layer:
\begin{align}
\pdv{E}{\Con i j 0} &= \pdv{E}{\Out i 0}\pdv{\Out i 0}{\Input i 0}\pdv{\Input i 0}{\Con j i 0}
\\
&= \underbrace{\left(\Out i 0 - \Target i \right)}_{\text{from} \ (\ref{cost_func_derivative})} \underbrace{\left(\Out i 0 \left(1 - \Out i 0\right) \right)}_{\text{from} \ (\ref{sigmoid_derivative})} \pdv{\Input i 0}{\Con j i 0} 
\\
\left( \pdv{\Input i m}{\Con j i m}\right) &= \pdv{}{\Con j i m}\left(\Input i m = \sum_j\Weight j i m\Out j {m+1} \right) = \pdv{}{\Con j i m} \left(\Con j i m + k \right) = 1\label{dxdc} 
\\
\pdv{E}{\Con j i 0}&= \left(\Out i 0 - \Target i \right) \left(\Out i 0 \left(1 - \Out i 0\right) \right)
\end{align}
Second derivative of the error with respect to a single input contribution $\Con j i 0$:
\begin{align}
\pdv[2]{E}{{\Con j i 0}} &=
\pdv{{^2}E}{{\Con j i 0}{\Out i 0}}
\pdv{\Out i 0}{{\Input i 0}} 
\underbrace{\pdv{\Input i 0}{\Con j i 0}}_{(\ref{dxdc})} 
+ 
\underbrace{\pdv{E}{{\Out i 0}}}_{(\ref{cost_func_derivative})}
\pdv{{^2}{\Out i 0}}{{\Con j i 0}{\Input i 0}} 
\underbrace{\pdv{{\Input i 0}}{{\Con j i 0}}}_{(\ref{dxdc})} 
+ 
\underbrace{\pdv{E}{{\Out i 0}}
\pdv{{\Out i 0}}{{\Input i 0}}
\pdv[2]{{\Input i 0}}{{\Con j i 0}}}_{\pdv{{\Input i 0}}{{\Con j i 0}} = 1\text{, so } \pdv[2]{{\Input i 0}}{{\Con j i 0}} = 0}\\
&= \left(\Out i 0 - \Target i \right)\
\label{zeroterm}
\end{align}
%& 

%Term \ref{zeroterm} is just zero, of course. Then, plugging in...
%
%\begin{align}
%\pdv[2]{E}{\Con i j 0} &= \pdv{}{\Con i j 0}\left(\Out i 0 - \Target i \right) \pdv{\Out i 0}{\Input i 0} + \left(\Out i 0 - \Target i \right) \pdv{}{\Con j i 0} \pdv{\Out i 0}{\Input i 0}\\
%&= \left(\pdv{\Out i 0}{\Input i j 0}\right)^2 + \left(\Out i 0 - \Target i \right) \sigma''\left(\Input i 0\right)\\
%&= \left(\sigma'\left(\Input i 0\right)\right)^2 + \left(\Out i 0 - \Target i \right) \sigma''\left(\Input i 0\right)
%\end{align}
%
%Moving on to the next layer, we have:
%\begin{align}
%\pdv{E}{\Out j 1} &= \pdv{E}{\Out i 0}\pdv{\Out i 0}{\Input i 0}\pdv{\Input i 0}{\Con j i 0}\pdv{\Con j i 0}{\Out j 1}
%= \left(\Out i 0 - \Target i\right)\sigma'(\Input i 0)\Weight j i 0
%\end{align}
%
%\begin{align}
%\pdv[2]{E}{\Out j 1} &= \pdv[2]{E}{\Out j 1}{\Out i 0}
%\cdot\pdv{\Out i 0}{\Input i 0}\pdv{\Input i 0}{\Con j i 0}
%\cdot\pdv{\Con j i 0}{\Out j 1}
%+ \pdv{E}{\Out i 0}
%\cdot\pdv[2]{\Out i 0}{\Out j 1}{\Input i 0}
%\cdot\pdv{\Input i 0}{\Con j i 0}
%\cdot\pdv{\Con j i 0}{\Out j 1} \notag \\
%&\quad+ \pdv{E}{\Out i 0}
%\cdot\pdv{\Out i 0}{\Input i 0}
%\cdot\pdv[2]{\Input i 0}{\Out j 1}{\Con j i 0}
%\cdot\pdv{\Con j i 0}{\Out j 1}
%+ \pdv{E}{\Out i 0}
%\cdot\pdv{\Out i 0}{\Input i 0}
%\cdot\pdv{\Input i 0}{\Con j i 0}
%\cdot\pdv[2]{\Con j i 0}{\Out j 1} \\
%&= \left(\sigma'(\Input i 0)\Weight j i 0\right)^2 + \left(\Out i 0 - \Target i\right)\sigma''(\Input i 0)\Weight j i 0^2
%\end{align}
