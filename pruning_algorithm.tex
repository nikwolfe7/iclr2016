\subsection{Pruning Algorithm: The Gain-Switch}
We propose the Gain-switch algorithm for pruning an optimally trained neural network here. We define gain as the quadratic error approximation $\Delta E_{k}^2$ we got from the Taylor Series.

The first step is to  decide a stopping criterion. This can vary depending on the application but some intuitive stopping criteria can be the maximum number of neurons to remove, percentage scaling needed, maximum allowable accuracy drop etc. The Gain-Switch algorithm performs the pruning in a greedy manner. It performs a forward propagation followed by a second-order back-propagation and collects the linear and quadratic gradients. It is to be noted that there is no weight update taking place during the back-propagation step as the network is already trained.This step is only used to collect the gradients. The algorithm then ranks all the neurons in the network based on their respective gain values and removes the neuron with the least value of the gain. This whole process is repeated until the stopping criterion is met.

\begin{algorithm}[H]
 \KwData{optimally trained network, training set}
 \KwResult{A pruned network after applying the Gain-switch algorithm}
 initialize and define stopping criterion \;
 \While{stopping criterion is not met}{
  perform forward propagation over the training set \;
  perform second-order back-propagation without updating weights and collect linear and quadratic gradients \;
  $gain$ = $\Delta E_{k}^2$ \;
  rank the remaining neurons based on $gain$ \;
  remove the neuron with the least value of $gain$ \;
 }
 \caption{The proposed Gain-switch pruning algorithm}
\end{algorithm}

The advantage of taking a greedy approach is that while removing the neurons, we are take into account the dependencies the neurons might have with one another. A negative value of the gain will indicate a neuron contributing to the output of another neuron in an inhibiting way. As mentioned in \cite{â€¢}, these 